{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Analysis of Filipino Cuisine Ingredients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read JSON files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def load_recipes(path=\"data/recipes\"):\n",
    "    \"\"\"Combines all recipe data into a single list.\"\"\"\n",
    "    files = [file for file in os.listdir(path)]\n",
    "\n",
    "    recipes = []\n",
    "    for file in files:\n",
    "        file_path = os.path.join(path, file)\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            recipe_data = json.load(f)\n",
    "            recipes.extend(recipe_data)\n",
    "\n",
    "    return recipes\n",
    "\n",
    "\n",
    "recipes = load_recipes()\n",
    "recipes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert JSON to DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "recipe_df = pd.DataFrame(recipes)\n",
    "recipe_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display some statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_ingredient_count_distribution(ingredient_series, figsize=(8, 3)):\n",
    "    \"\"\"Plots the distribution of the number of ingredients in recipes.\"\"\"\n",
    "    counts = ingredient_series.dropna().apply(len)\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.histplot(counts, kde=True, binwidth=1)\n",
    "    plt.title(f\"Number of ingredients in {len(counts)} recipes\")\n",
    "    plt.xlabel(\"Number of ingredients\")\n",
    "    plt.ylabel(\"Number of recipes\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_common_ingredients(\n",
    "    ingredient_series, n=30, most_common=True, figsize=(8, 7)\n",
    "):\n",
    "    \"\"\"Plots the most or least common ingredients.\"\"\"\n",
    "    all_ingredients = ingredient_series.dropna().explode()\n",
    "    ingredient_counts = all_ingredients.value_counts()\n",
    "\n",
    "    if most_common:\n",
    "        top_n_ingredients = ingredient_counts.head(n)\n",
    "    else:\n",
    "        top_n_ingredients = ingredient_counts.tail(n)\n",
    "\n",
    "    ylabels = [\n",
    "        f\"{i[:20]:>20}{'...' if len(i) > 20 else ''}\"\n",
    "        for i in top_n_ingredients.index\n",
    "    ]\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.barplot(x=top_n_ingredients.values, y=ylabels)\n",
    "    indicator = \"Most\" if most_common else \"Least\"\n",
    "    plt.title(f\"Top {n} {indicator} Common Ingredients\")\n",
    "    plt.xlabel(\"Number of Recipes\")\n",
    "    plt.ylabel(\"Ingredient\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_ingredient_count_distribution(recipe_df.ingredients)\n",
    "plot_common_ingredients(recipe_df.ingredients, n=30, most_common=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0. Remove recipes with no ingredients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_empty_ingredients(recipe_df, column=\"ingredients\"):\n",
    "    \"\"\"Drops rows with empty ingredients.\"\"\"\n",
    "    new_df = recipe_df.dropna(subset=[column])\n",
    "    print(f\"Dropped {recipe_df.shape[0] - new_df.shape[0]} rows.\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "recipe_df = drop_empty_ingredients(recipe_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ingredients(ingredient_series, path):\n",
    "    \"\"\"Saves the ingredients to a TXT file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    all_ingredients = ingredient_series.dropna().explode()\n",
    "    data = sorted(all_ingredients.unique().astype(str))\n",
    "\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for ingredient in data:\n",
    "            file.write(f\"{ingredient.strip()}\\n\")\n",
    "\n",
    "\n",
    "save_ingredients(\n",
    "    recipe_df.ingredients,\n",
    "    path=\"data/preprocess_output/0_raw.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Clean ingredient format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def remove_parentheses(ingredient):\n",
    "    \"\"\"Removes parentheses and content from ingredient strings.\"\"\"\n",
    "    return re.sub(r\" ?\\([^)]+\\)\", \"\", ingredient)\n",
    "\n",
    "\n",
    "def select_first_option(ingredient):\n",
    "    \"\"\"Selects the first option in a string with multiple options.\"\"\"\n",
    "    return ingredient.split(\" or \")[0]\n",
    "\n",
    "\n",
    "def clean_ingredient(ingredient):\n",
    "    \"\"\"Cleans an ingredient string.\"\"\"\n",
    "    ingredient = remove_parentheses(ingredient)\n",
    "    ingredient = select_first_option(ingredient)\n",
    "    return ingredient.lower().strip()\n",
    "\n",
    "\n",
    "recipe_df[\"cleaned\"] = recipe_df.ingredients.progress_apply(\n",
    "    lambda x: [cleaned for i in x if (cleaned := clean_ingredient(i))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ingredients(\n",
    "    recipe_df.cleaned,\n",
    "    path=\"data/preprocess_output/1_cleaned.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Lemmatize words and remove non-ingredient words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "def correct_noun_pos_tags(token):\n",
    "    \"\"\"Corrects the POS tags of specific tokens.\"\"\"\n",
    "    outliers = [\"cauliflower\", \"baking\"]\n",
    "    if token.text in outliers:\n",
    "        token.pos_ = \"NOUN\"\n",
    "    return token\n",
    "\n",
    "\n",
    "def lemmatize_nouns(ingredient):\n",
    "    \"\"\"Lemmatizes nouns in an ingredient string.\"\"\"\n",
    "    doc = nlp(ingredient)\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        token = correct_noun_pos_tags(token)\n",
    "        if (\n",
    "            token.is_alpha\n",
    "            and not token.is_stop\n",
    "            and token.pos_ in [\"NOUN\", \"PROPN\"]\n",
    "        ):\n",
    "            lemmas.append(token.lemma_)\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "\n",
    "recipe_df[\"lemmatized\"] = recipe_df.cleaned.progress_apply(\n",
    "    lambda x: [lemmatized for i in x if (lemmatized := lemmatize_nouns(i))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ingredients(\n",
    "    recipe_df.lemmatized,\n",
    "    path=\"data/preprocess_output/2_lemmatized.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Remove stop words related to culinary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"data/preprocess_input/stopwords.txt\"\n",
    "with open(input_file, encoding=\"utf-8\") as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "\n",
    "def filter_stopwords(text):\n",
    "    \"\"\"Filter out stopwords from a string.\"\"\"\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        if word not in stopwords:\n",
    "            words.append(word)\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "recipe_df[\"filtered\"] = recipe_df.lemmatized.progress_apply(\n",
    "    lambda x: [filtered for i in x if (filtered := filter_stopwords(i))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ingredients(\n",
    "    recipe_df.filtered,\n",
    "    path=\"data/preprocess_output/3_filtered.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Standardize ingredient names at word level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"data/preprocess_input/thesaurus.json\"\n",
    "with open(input_file, encoding=\"utf-8\") as file:\n",
    "    thesaurus = json.load(file)\n",
    "\n",
    "\n",
    "def standardize_at_phrase_level(ingredient):\n",
    "    \"\"\"Replace words in an ingredient with standard names.\"\"\"\n",
    "    words = []\n",
    "    for word in ingredient.split():\n",
    "        words.append(thesaurus.get(word, word))\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "recipe_df[\"standardized_word\"] = recipe_df.filtered.progress_apply(\n",
    "    lambda x: [\n",
    "        common_name\n",
    "        for i in x\n",
    "        if (common_name := standardize_at_phrase_level(i))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ingredients(\n",
    "    recipe_df.standardized_word,\n",
    "    path=\"data/preprocess_output/4_standardized_word.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Remove duplicate terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(text):\n",
    "    \"\"\"Remove duplicate terms while preserving order.\"\"\"\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for word in text.split():\n",
    "        if word not in seen:\n",
    "            unique.append(word)\n",
    "            seen.add(word)\n",
    "    return \" \".join(unique)\n",
    "\n",
    "\n",
    "recipe_df[\"unique\"] = recipe_df.standardized_word.progress_apply(\n",
    "    lambda x: [unique for i in x if (unique := remove_duplicates(i))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ingredients(\n",
    "    recipe_df.unique,\n",
    "    path=\"data/preprocess_output/5_unique.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Standardize ingredient names at phrase level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_at_phrase_level(ingredient):\n",
    "    \"\"\"Replace the whole ingredient string with a standard name.\"\"\"\n",
    "    return thesaurus.get(ingredient, ingredient)\n",
    "\n",
    "\n",
    "recipe_df[\"standardized_phrase\"] = recipe_df.unique.progress_apply(\n",
    "    lambda x: [\n",
    "        standardized\n",
    "        for i in x\n",
    "        if (standardized := standardize_at_phrase_level(i))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ingredients(\n",
    "    recipe_df.standardized_phrase,\n",
    "    path=\"data/preprocess_output/6_standardized_phrase.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Break down ingredients into basic components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"data/preprocess_input/components.json\"\n",
    "with open(input_file, encoding=\"utf8\") as file:\n",
    "    ingredient_components = json.load(file)\n",
    "\n",
    "\n",
    "def break_down_ingredient(ingredient):\n",
    "    \"\"\"Breaks down an ingredient into its components.\"\"\"\n",
    "    components = ingredient_components.get(ingredient, ingredient)\n",
    "    if isinstance(components, str):\n",
    "        return [components]\n",
    "    return components\n",
    "\n",
    "\n",
    "recipe_df[\"components\"] = recipe_df.standardized_phrase.progress_apply(\n",
    "    lambda x: [component for i in x for component in break_down_ingredient(i)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ingredients(\n",
    "    recipe_df.components,\n",
    "    path=\"data/preprocess_output/7_components.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Remove very common ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "very_common_ingredients = [\n",
    "    \"salt\",\n",
    "    \"pepper\",\n",
    "    \"sugar\",\n",
    "    \"oil\",\n",
    "    \"water\",\n",
    "    \"garlic\",\n",
    "    \"onion\",\n",
    "]\n",
    "\n",
    "# Remove very common ingredients\n",
    "recipe_df[\"filtered_common\"] = recipe_df.components.progress_apply(\n",
    "    lambda x: [i for i in x if i not in very_common_ingredients]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ingredients(\n",
    "    recipe_df.filtered_common,\n",
    "    path=\"data/preprocess_output/8_filtered_common.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save preprocessed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_df.to_csv(\n",
    "    \"data/preprocessed_recipes.csv\",\n",
    "    index=False,\n",
    "    columns=[\"name\", \"filtered_common\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert DataFrame to one-hot encoded format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "one_hot = mlb.fit_transform(recipe_df.filtered_common)\n",
    "one_hot_df = pd.DataFrame(one_hot, columns=mlb.classes_)\n",
    "\n",
    "one_hot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_df.to_csv(\n",
    "    \"data/one_hot_encoded_recipes.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "one_hot_df = pd.read_csv(\"data/one_hot_encoded_recipes.csv\")\n",
    "one_hot_df = one_hot_df.astype(bool)\n",
    "one_hot_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find frequent itemsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "frequent_itemsets = apriori(\n",
    "    one_hot_df,\n",
    "    min_support=0.01, #  Lower this value to get more frequent itemsets\n",
    "    use_colnames=True,\n",
    ")\n",
    "\n",
    "frequent_itemsets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate association rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "rules_df = association_rules(\n",
    "    frequent_itemsets,\n",
    "    metric=\"confidence\",\n",
    "    min_threshold=0.5,\n",
    ")\n",
    "\n",
    "cols = [\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"]\n",
    "rules_df[cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_df.to_csv(\n",
    "    \"data/association_rules.csv\",\n",
    "    index=False,\n",
    "    columns=[\"antecedents\", \"consequents\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and exploring rules\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "filtered_rules = rules[(rules[\"lift\"] >= 1.2) & (rules[\"confidence\"] >= 0.6)]\n",
    "filtered_rules.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_df[\"antecedents\"] = rules_df[\"antecedents\"].apply(\n",
    "    lambda x: \", \".join(list(x))\n",
    ")\n",
    "rules_df[\"consequents\"] = rules_df[\"consequents\"].apply(\n",
    "    lambda x: \", \".join(list(x))\n",
    ")\n",
    "\n",
    "edges = [\n",
    "    (ant, cons)\n",
    "    for ant, cons in zip(rules_df[\"antecedents\"], rules_df[\"consequents\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Node and Edge Information on Hover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G)  # Node positions\n",
    "edge_x = []\n",
    "edge_y = []\n",
    "for edge in G.edges():\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "    edge_x.extend([x0, x1, None])\n",
    "    edge_y.extend([y0, y1, None])\n",
    "\n",
    "node_x = [pos[node][0] for node in G.nodes()]\n",
    "node_y = [pos[node][1] for node in G.nodes()]\n",
    "node_adjacencies = []\n",
    "node_text = []\n",
    "for node in G.nodes():\n",
    "    adjacents = G.adj[node]\n",
    "    connections = [f\"{node} → {neighbor}\" for neighbor in adjacents]\n",
    "    node_adjacencies.append(len(adjacents))\n",
    "    node_text.append(\"Connections:<br>\" + \"<br>\".join(connections))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Edge and Node Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "edge_trace = go.Scatter(\n",
    "    x=edge_x,\n",
    "    y=edge_y,\n",
    "    line=dict(width=0.5, color=\"#888\"),\n",
    "    hoverinfo=\"none\",\n",
    "    mode=\"lines\",\n",
    ")\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x,\n",
    "    y=node_y,\n",
    "    mode=\"markers\",\n",
    "    hoverinfo=\"text\",\n",
    "    text=node_text,\n",
    "    marker=dict(\n",
    "        showscale=True,\n",
    "        colorscale=\"YlGnBu\",\n",
    "        reversescale=True,\n",
    "        color=node_adjacencies,\n",
    "        size=10,\n",
    "        colorbar=dict(\n",
    "            thickness=15,\n",
    "            title=\"Node Connections\",\n",
    "            xanchor=\"left\",\n",
    "            titleside=\"right\",\n",
    "        ),\n",
    "        line_width=1,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    data=[edge_trace, node_trace],\n",
    "    layout=go.Layout(\n",
    "        showlegend=False,\n",
    "        hovermode=\"closest\",\n",
    "        margin=dict(b=20, l=20, r=20, t=20),\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    ),\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
